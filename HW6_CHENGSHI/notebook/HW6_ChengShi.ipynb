{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c24ea9c",
   "metadata": {},
   "source": [
    "Cheng Shi;\n",
    "USCID: 8556138235;\n",
    "Github:CaronShi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35bbb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788e859",
   "metadata": {},
   "source": [
    "1. Tree-Based Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6033372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a)\n",
    "\n",
    "train = pd.read_csv(\"../data/aps_failure_training_set.csv\", skiprows=20).replace(\"na\", np.nan)\n",
    "test = pd.read_csv(\"../data/aps_failure_test_set.csv\", skiprows=20).replace(\"na\", np.nan)\n",
    "\n",
    "#seperate dataset \n",
    "train_x = train.iloc[:, 1:]#datapoint without class\n",
    "train_y = train.iloc[:, 0]#class\n",
    "test_x = test.iloc[:,1:]\n",
    "test_y = test.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfbffac",
   "metadata": {},
   "source": [
    "(b)Data Preparation\n",
    "\n",
    "i.Research what types of techniques are usually used for dealing with data with missing values. Pick at least one of them and apply it to this data in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c104b",
   "metadata": {},
   "source": [
    "According to what we learned in class, there are some of the methods dealing with missing value. 1)replace missing values by median/mean or other meaningful statistics. 2)using some models to predict the misssing value 3)simply skip the missing values 4)replacing by most frequent values. In this data set, since we have a lot of missing values, it's not optimal to skip those missing data, I decide to choose using median replacing methods and most frequent number replacing method in this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy = \"median\")\n",
    "train_x = pd.DataFrame(imp.fit_transform(train_x), columns = train_x.columns)\n",
    "test_x = pd.DataFrame(imp.fit_transform(test_x), columns = test_x.columns)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab2545",
   "metadata": {},
   "source": [
    "ii.For each of the 170 features, calculate the coefficient of variation CV=s/m,where s is sample standard deviation and m is sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = train_x.std() / train_x.mean()\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36402bc1",
   "metadata": {},
   "source": [
    "iii.Plot a correlation matrix for your features using pandas or any other tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_df = train_x.columns\n",
    "cols = []\n",
    "for i in cols_df:\n",
    "    cols.append(i)\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.matshow(train_x.corr(),fignum= fig.number)\n",
    "plt.xticks(range(len(cols)),cols,rotation='vertical')\n",
    "plt.yticks(range(len(cols)),cols)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec018239",
   "metadata": {},
   "source": [
    "iv.Pick √170 features with highest CV, and make scatter plots and box plots for them, similar to those on p.129 of ISLR. Can you draw conclusions about significance of those features, just by the scatter plots? This does not mean that you will only use those features in the following questions. We picked them only for visualization.\n",
    "\n",
    "Yes, from the graphs, we can see a positive relationship in au000 and as000 so we may say au000 and as000 are significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10373b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pick = np.sqrt(170)\n",
    "pick = np.floor(pick)\n",
    "cv_pick = cv.sort_values(ascending=False) [:int(pick)]\n",
    "cv_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b38f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pick = np.sqrt(170)\n",
    "pick = np.floor(pick)\n",
    "\n",
    "cv_pick = cv.sort_values(ascending=False) [:int(pick)]\n",
    "df_pick = pd.DataFrame(train_x, columns = cv_pick.keys())\n",
    "df_pick.loc[:, \"class\"] = train_y\n",
    "sns.pairplot(data = df_pick, kind = \"scatter\", diag_kind= 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots\n",
    "for y in list(cv_pick.keys()):\n",
    "    fig = plt.figure(figsize = (3, 3))\n",
    "    sns.boxplot(data = df_pick, x=\"class\", y=y, showfliers = False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = df_pick.boxplot(rot=90, fontsize=10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90db51",
   "metadata": {},
   "source": [
    "v.Determine the number of positive and negative data. Is this data set imbalanced?\n",
    "\n",
    "ANS:\n",
    "From the next cell, we can see that the  number of negatives is far more than positives' in both train(59000 vs 1000) and test(15625 vs 375) dataset, so we could conclude the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917197ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\",train_y.value_counts())\n",
    "\n",
    "print(\"Test:\",test_y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d26ef",
   "metadata": {},
   "source": [
    "(c)Train a random forest to classify the data set. Do NOT compensate for class imbalance in the data set. \n",
    "\n",
    "Calculate the confusion matrix, ROC, AUC, and misclassification for training and test sets and report them (You may use pROC package). Calculate Out of Bag error estimate for your random forset and compareit to the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a1040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a random forest\n",
    "y_true = train_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "#test_y = test_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "clf = RandomForestClassifier(oob_score = True)\n",
    "clf.fit(train_x, train_y)\n",
    "y_pred = clf.predict(train_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab757f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = y_pred.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(train_y, y_pred)\n",
    "print('confusion_matrix:', cm)\n",
    "\n",
    "#misclassification\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "misclassfication = fp+fn\n",
    "print('misclassification:',misclassfication)\n",
    "\n",
    "print('misclassification rate :',misclassfication/(tn+fp+fn+tp) )\n",
    "\n",
    "#Out of Bag error estimate\n",
    "oob_error = 1 - clf.oob_score_\n",
    "print('oob error:',oob_error)\n",
    "\n",
    "#ROC\n",
    "scores = clf.predict_proba(train_x)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "#AUC\n",
    "auc = roc_auc_score(y_true, scores)\n",
    "print('aur:', auc)\n",
    "plt.plot(fpr, tpr,label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color=\"black\",label = 'AUC')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51bb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test a random forest\n",
    "test_y = test_y.replace(0,\"neg\").replace(1,\"pos\")\n",
    "y_pred = clf.predict(test_x)\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "print('confusion_matrix:', cm)\n",
    "\n",
    "#misclassification\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "misclassfication = fp+fn\n",
    "print('misclassification:',misclassfication )\n",
    "print('misclassification rate :',misclassfication/(tn+fp+fn+tp) )\n",
    "#Out of Bag error estimate\n",
    "oob_error = 1 - clf.oob_score_\n",
    "print('oob error:',oob_error)\n",
    "\n",
    "#ROC\n",
    "y_true= test_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "scores = clf.predict_proba(test_x)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "#AUC\n",
    "auc = roc_auc_score(y_true, scores)\n",
    "print('aur:', auc)\n",
    "plt.plot(fpr, tpr,label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color=\"black\",label = 'AUC')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496a69f",
   "metadata": {},
   "source": [
    "(d) Research how class imbalance is addressed in random forests. Compensate for class imbalance in your random forest and repeat 1c. Compare the results withthose of 1c.4\n",
    "\n",
    "ANS:\n",
    "Random forests address imbalance with class weighting and random undersampling.\n",
    "\n",
    "reference: https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994590a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(oob_score = True,class_weight='balanced')\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "y_pred = clf.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d316f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "y_true = train_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(train_y, y_pred)\n",
    "print('confusion_matrix:', cm)\n",
    "\n",
    "#misclassification\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "misclassfication = fp+fn\n",
    "print('misclassification:',misclassfication)\n",
    "\n",
    "print('misclassification rate :',misclassfication/(tn+fp+fn+tp) )\n",
    "\n",
    "#Out of Bag error estimate\n",
    "oob_error = 1 - clf.oob_score_\n",
    "print('oob error:',oob_error)\n",
    "\n",
    "#ROC\n",
    "scores = clf.predict_proba(train_x)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "#AUC\n",
    "auc = roc_auc_score(y_true, scores)\n",
    "print('aur:', auc)\n",
    "plt.plot(fpr, tpr,label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color=\"black\",label = 'AUC')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd824310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test a random forest\n",
    "y_true= test_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "y_pred = clf.predict(test_x)\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "print('confusion_matrix:', cm)\n",
    "\n",
    "#misclassification\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "misclassfication = fp+fn\n",
    "print('misclassification:',misclassfication )\n",
    "print('misclassification rate :',misclassfication/(tn+fp+fn+tp) )\n",
    "#Out of Bag error estimate\n",
    "oob_error = 1 - clf.oob_score_\n",
    "print('oob error:',oob_error)\n",
    "\n",
    "#ROC\n",
    "scores = clf.predict_proba(test_x)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "#AUC\n",
    "auc = roc_auc_score(y_true, scores)\n",
    "print('aur:', auc)\n",
    "plt.plot(fpr, tpr,label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color=\"black\",label = 'AUC')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479849c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = {'oob error':[0.00633,0.00765,0.00633,0.00765],'misclassification':[0,0,126,182]\n",
    "    }\n",
    "\n",
    "compare = pd.DataFrame(data = d)\n",
    "compare.index = ['train unbalance','train balance','test unbalance','test balance']\n",
    "print(compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bab84d9",
   "metadata": {},
   "source": [
    "From the table above, we can observe the difference between class balanced and unbalanced, and for the obb error, the balanced one has a larger error; for the misclassification number, the balanced one for test set also get larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669049d",
   "metadata": {},
   "source": [
    "(e)XGBoost and Model Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2ae6a",
   "metadata": {},
   "source": [
    "use XGBoost to fit the model tree. \n",
    " \n",
    "determine α(the regularization term) using cross-validation. \n",
    " \n",
    "Train the model for the APS data set without compensation for class imbalance. \n",
    " \n",
    "Use one of 5 fold, 10 fold, and leave-one-out cross validation methods to estimate the error of your trained model and compare it with the test error. \n",
    " \n",
    "Report the Confusion Matrix, ROC,and AUC for training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2acc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = train_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "\n",
    "#use XGBoost to fit the model tree\n",
    "xgb = XGBClassifier(objective= \"binary:logistic\", use_label_encoder=False, )\n",
    "\n",
    "#5-fold\n",
    "param_grid = {'reg_alpha':[1e-3, 1e-2, 0.1, 1, 10, 100]}\n",
    "gsearch = GridSearchCV(estimator = xgb, param_grid = param_grid, scoring = \"roc_auc\", cv = 5)\n",
    "gsearch.fit(train_x, y_true, eval_metric = \"logloss\")\n",
    "print('the best score for 5-fold cv:',gsearch.best_score_)\n",
    "print('the best alpha for 5-fold cv: ',gsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be5999",
   "metadata": {},
   "source": [
    "the best score for 5-fold cv: 0.9907702542372882\n",
    "the best alpha for 5-fold cv:  {'reg_alpha': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f9e2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true= train_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "test_y = test_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "xgb_model = XGBClassifier(objective= \"binary:logistic\", random_state = 0,  eval_metric = \"logloss\",use_label_encoder = False, reg_alpha = 10)\n",
    "#xgbc.fit(train_x, y_true)\n",
    "xgb_model.fit(train_x, y_true)\n",
    "predict_y = xgb_model.predict(test_x)\n",
    "test_error = 1 - accuracy_score(test_y, predict_y)\n",
    "print(\"test error:\", test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa38968",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_score = 1-test_error\n",
    "d = {'test error':[xgb_score,0.990770],'misclas':[0,0]}\n",
    "\n",
    "compare = pd.DataFrame(data = d)\n",
    "compare.index = ['xgbc','5-fold cv']\n",
    "print(compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e778",
   "metadata": {},
   "source": [
    "From the table above, the test score is bigger than the 5-fold score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3183c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train  dataset\n",
    "y_true = train_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "test_y = test_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "y_pred = xgb_model.predict(train_x)\n",
    "\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('confusion_matrix:', cm)\n",
    "\n",
    "\n",
    "#ROC\n",
    "scores = clf.predict_proba(train_x)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "#AUC\n",
    "auc = roc_auc_score(y_true, scores)\n",
    "print('aur:', auc)\n",
    "plt.plot(fpr, tpr,label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color=\"black\",label = 'AUC')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset\n",
    "y_true= test_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "y_pred = xgb_model.predict(test_x)\n",
    "\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('confusion_matrix:', cm)\n",
    "\n",
    "#ROC\n",
    "scores = clf.predict_proba(test_x)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "\n",
    "#AUC\n",
    "auc = roc_auc_score(y_true, scores)\n",
    "print('aur:', auc)\n",
    "plt.plot(fpr, tpr,label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color=\"black\",label = 'AUC')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a25e9",
   "metadata": {},
   "source": [
    "(f)Use SMOTE (Synthetic Minority Over-sampling Technique) to pre-process your data to compensate for class imbalance. Train XGBoost with L1-penalized logistic regression at each node using the pre-processed data and repeat 1e. Do not forget that there is a right and a wrong way of cross validation here. Comparethe uncompensated case with SMOTE case.\n",
    "\n",
    "reference https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = imbpipeline(steps = [['smote', SMOTE(random_state = 0)], ['scaler', MinMaxScaler()],\n",
    "                                ['classifier', LogisticRegression(random_state = 0, max_iter=1000)]])\n",
    "param_grid = {\"classifier__C\":[1e-3, 1e-2, 0.1, 1, 10, 100]}\n",
    "stratified_kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "\n",
    "train_y_num = train_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "\n",
    "#trainy = train_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "#trainx = train_x.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "# 5-fold cv with SMOTE\n",
    "grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = \"roc_auc\", cv = stratified_kfold, n_jobs=-1)\n",
    "grid_search.fit(train_x, train_y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "gparam = grid_search.best_params_, \n",
    "gs_score = grid_search.best_score_\n",
    "gtest_score = grid_search.score(test_x,test_y)\n",
    "print(gparam,'best score:', gs_score,'test score:', gtest_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936e5af",
   "metadata": {},
   "source": [
    "the test score 0.9891758, which is smaller 5-fold cv test error 0.99077 and xgbc test error 0.993437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(train_x)\n",
    "y_true= train_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('confusion_matrix:', cm)\n",
    "\n",
    "\n",
    "#ROC\n",
    "scores_tr = grid_search.predict_proba(train_x)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores_tr)\n",
    "\n",
    "#AUC\n",
    "auc = roc_auc_score(y_true, scores_tr)\n",
    "print(auc)\n",
    "plt.plot(fpr, tpr,label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color=\"black\",label = 'AUC')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9b12b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test\n",
    "y_pred = grid_search.predict(test_x)\n",
    "y_true= test_y.replace(\"neg\", 0).replace(\"pos\", 1)\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('confusion_matrix:', cm)\n",
    "\n",
    "\n",
    "#ROC\n",
    "scores_test = grid_search.predict_proba(test_x)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_true, scores_test)\n",
    "\n",
    "#AUC\n",
    "auc = roc_auc_score(y_true, scores_test)\n",
    "print(auc)\n",
    "plt.plot(fpr, tpr,label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color=\"black\",label = 'AUC')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ec18d",
   "metadata": {},
   "source": [
    "ISLR 6.6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135a873",
   "metadata": {},
   "source": [
    "(a) iv, Steadily decrease; because we need increase s and make models more flexiable.\n",
    "\n",
    "(b) ii,Decrease initially, and then eventually start increasing in a U shape; because when models more flexiable, RSS will reduce then increase while overfitting\n",
    "(c) iii, Steadily increase; variance will increase with flexiblility.\n",
    "(d) iv, Steadily decrease; bias decreased with the increasing flexiblility.\n",
    "(e) v Remain constant, because it is independent and not be affected by other factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910e5988",
   "metadata": {},
   "source": [
    "ISLR 6.6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e68059",
   "metadata": {},
   "source": [
    "(a)ridge = (y1−β1x11−β2x12)^2+(y2−β1x21−β2x22)^2+λ(β1^2+β2^2)\n",
    "\n",
    "(b)because we know  x11=x12,x21=x22,y1+y2=0 and x11+x21=0 and x12+x22=0,\n",
    "x11=x12 =-x21=-x22=x, y1=-y2=y,\n",
    "simplied formulas: 2[y-x(β1+β2)]^2 + λ(β1^2+β2^2) = 2y^2-4yxβ1-4yxβ2+ [x(β1+β2)]^2 +λ(β1^2+β2^2) = 2y^2-4yxβ1-4yxβ2+ x^2 *(β1^2+2β1β2+β2^2)+λ(β1^2+β2^2)  = 0 \n",
    "\n",
    "λ2β1=4x[y−x(β1+β2)]\n",
    "λ2β2=4x[x−y(β1+β2)]\n",
    "so β1=β2\n",
    "\n",
    "(c)lasso = (y1−β1x11−β2x12)^2+(y2−β1x21−β2x22)^2+λ(|β1|+|β2|)\n",
    "\n",
    "(d)the only difference between lasso formula and ridge's is the loss penalty function after λ, so we could easily find the simplied formula for lasso is 2[y-x(β1+β2)]^2 + λ(|β1|+|β2|) , opt. = β1+β2= y/x= s, if in f(x,y) = 4x[x−y(β1+β2)], β1+β2 is +, f(x,y) is -, and β1+β2 is -, f(x,y) is +, so the lasso doesn't have only one solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6e881",
   "metadata": {},
   "source": [
    "ISLR 8.4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db35cb8a",
   "metadata": {},
   "source": [
    "(1) Majority vote: it's red because the number is 6 out of 10;\n",
    "(2)Avg: the mean is 0.45 which belongs to green.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26313f3",
   "metadata": {},
   "source": [
    "ISLR 9.7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376964b6",
   "metadata": {},
   "source": [
    "(a) We are given n = 7 observations in p = 2 dimensions. For each observation, there is an associated class label. Sketch the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c34953",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [3, 2, 4, 1, 2, 4, 4]\n",
    "x2 = [4, 2, 4, 4, 1, 3, 1]\n",
    "y = [\"red\", \"red\", \"red\", \"red\", \"blue\", \"blue\", \"blue\"]\n",
    "plt.scatter(x1, x2, c = y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4d3e9",
   "metadata": {},
   "source": [
    "(b) Sketch the optimal separating hyperplane, and provide the equation for this hyperplane (of the form (9.1)). \n",
    "\n",
    "ANS:X1−X2+0.5 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x1, x2, c = y)\n",
    "x = np.arange(0, 5, 0.1)\n",
    "y = x - 0.5\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f6b0c",
   "metadata": {},
   "source": [
    "(c) Describe the classification rule for the maximal margin classifier. It should be something along the lines of “Classify to Red if β0+β1X1+β2X2>0, and classify to Blue otherwise.” Provide the values for β0,β1,β2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6559916",
   "metadata": {},
   "source": [
    "if X2 - X1 + 0.5 > 0, it's in red class, or in blue class when < 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2e3c2d",
   "metadata": {},
   "source": [
    "(d)On your sketch, indicate the margin for the maximal margin hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab94c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [\"red\", \"red\", \"red\", \"red\", \"blue\", \"blue\", \"blue\"]\n",
    "plt.plot([1,5],[0,4], linestyle = '--')\n",
    "plt.plot([0.5, 5], [0, 4.5])\n",
    "plt.plot([0,5],[0,5], linestyle = '--')\n",
    "\n",
    "plt.scatter(x1, x2, c = y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9fd7a7",
   "metadata": {},
   "source": [
    "(e) Indicate the support vectors for the maximal margin classifier.\n",
    "\n",
    "(f) Argue that a slight movement of the seventh observation would\n",
    "not affect the maximal margin hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386b53c",
   "metadata": {},
   "source": [
    "ANS: \n",
    "(e) (2,1), (2,2), (4,3), (4,4).\n",
    "\n",
    "(f) Obs(7) is (4,1) which is not support vectors so it would not affect the  maximal margin hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd536625",
   "metadata": {},
   "source": [
    "\n",
    "(g) Sketch a hyperplane that is not the optimal separating hyper-\n",
    "plane, and provide the equation for this hyperplane.\n",
    "\n",
    "(h) Draw an additional observation on the plot so that the two\n",
    "classes are no longer separable by a hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a82237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(g)\n",
    "plt.scatter(x1, x2, c = c)\n",
    "x = np.arange(0, 5, 0.1)\n",
    "y = 0.2*x + 0.5\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(h)\n",
    "x1 = [3, 2, 4, 1, 2, 4, 4,3]\n",
    "x2 = [4, 2, 4, 4, 1, 3, 1,1]\n",
    "y = [\"red\", \"red\", \"red\", \"red\", \"blue\", \"blue\", \"blue\",'red']\n",
    "plt.scatter(x1, x2, c = y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
